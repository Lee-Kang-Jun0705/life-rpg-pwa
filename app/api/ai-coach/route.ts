import { NextRequest, NextResponse } from 'next/server'
import { Stat } from '@/lib/types/dashboard'
import { GrowthAnalysis, ActivityPattern } from '@/lib/types/ai-coach'
import { AIMessage } from '@/types/ai'

// API ì œê³µì íƒ€ì…
type AIProvider = 'openai' | 'anthropic' | 'google' | 'groq' | 'custom'

interface AIConfig {
  provider: AIProvider
  apiKey: string
  model: string
  endpoint?: string
}

// í™˜ê²½ë³€ìˆ˜ì—ì„œ API ì„¤ì • ì½ê¸°
function getAIConfig(): AIConfig | null {
  const provider = process.env.NEXT_PUBLIC_AI_PROVIDER as AIProvider
  const apiKey = process.env.NEXT_PUBLIC_AI_API_KEY
  
  if (!provider || !apiKey) {
    return null
  }

  const configs: Record<AIProvider, Partial<AIConfig>> = {
    openai: {
      model: process.env.NEXT_PUBLIC_AI_MODEL || 'gpt-3.5-turbo',
      endpoint: 'https://api.openai.com/v1/chat/completions'
    },
    anthropic: {
      model: process.env.NEXT_PUBLIC_AI_MODEL || 'claude-3-sonnet-20240229',
      endpoint: 'https://api.anthropic.com/v1/messages'
    },
    google: {
      model: process.env.NEXT_PUBLIC_AI_MODEL || 'gemini-pro',
      endpoint: 'https://generativelanguage.googleapis.com/v1beta/models'
    },
    groq: {
      model: process.env.NEXT_PUBLIC_AI_MODEL || 'mixtral-8x7b-32768',
      endpoint: 'https://api.groq.com/openai/v1/chat/completions'
    },
    custom: {
      model: process.env.NEXT_PUBLIC_AI_MODEL || '',
      endpoint: process.env.NEXT_PUBLIC_AI_ENDPOINT || ''
    }
  }

  return {
    provider,
    apiKey,
    ...configs[provider]
  } as AIConfig
}

export async function POST(request: NextRequest) {
  try {
    const { message, stats, growthAnalyses, activityPattern } = await request.json() as {
      message: string
      stats: Stat[]
      growthAnalyses: GrowthAnalysis[]
      activityPattern: ActivityPattern
    }
    
    const config = getAIConfig()
    
    // API ì„¤ì •ì´ ì—†ìœ¼ë©´ ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ
    if (!config) {
      return NextResponse.json({
        response: generateRuleBasedResponse(message),
        provider: 'rule-based'
      })
    }

    const systemPrompt = `ë‹¹ì‹ ì€ Life RPG ì•±ì˜ ê°œì¸ ì„±ì¥ AI ì½”ì¹˜ì…ë‹ˆë‹¤. 
    ì‚¬ìš©ìì˜ í™œë™ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
    ì‘ë‹µì€ ì¹œê·¼í•˜ê³  ê²©ë ¤í•˜ëŠ” í†¤ìœ¼ë¡œ, ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”.`

    const userContext = `
    í˜„ì¬ ì‚¬ìš©ì ìƒíƒœ:
    - ì´ ë ˆë²¨: ${stats.reduce((sum, s) => sum + (s.level || 1), 0)}
    - ìŠ¤íƒ¯ë³„ ë ˆë²¨: ${stats.map(s => `${s.type} Lv.${s.level}`).join(', ')}
    - í™œë™ íŒ¨í„´: ì¼í‰ê·  ${activityPattern?.averageActivitiesPerDay.toFixed(1)}íšŒ, ì—°ì† ${activityPattern?.streakDays}ì¼
    - ì„±ì¥ ì¶”ì„¸: ${growthAnalyses.map(g => `${g.statType}: ${g.trend}`).join(', ')}
    
    ì‚¬ìš©ì ì§ˆë¬¸: ${message}
    `

    const messages: AIMessage[] = [
      { role: 'system' as const, content: systemPrompt },
      { role: 'user' as const, content: userContext }
    ]

    let aiResponse: string

    // ì œê³µìë³„ API í˜¸ì¶œ
    switch (config.provider) {
      case 'openai':
        aiResponse = await callOpenAI(config, messages)
        break
      case 'anthropic':
        aiResponse = await callAnthropic(config, messages)
        break
      case 'google':
        aiResponse = await callGoogle(config, messages)
        break
      case 'groq':
        aiResponse = await callGroq(config, messages)
        break
      case 'custom':
        aiResponse = await callCustomAPI(config, messages)
        break
      default:
        aiResponse = generateRuleBasedResponse(message)
    }

    return NextResponse.json({ 
      response: aiResponse,
      provider: config.provider,
      model: config.model 
    })
  } catch (error) {
    console.error('AI Coach API error:', error)
    
    // ì—ëŸ¬ ë°œìƒì‹œ ê·œì¹™ ê¸°ë°˜ í´ë°±
    return NextResponse.json({
      response: generateRuleBasedResponse(''),
      provider: 'rule-based',
      error: true
    })
  }
}

// ê° ì œê³µìë³„ API í˜¸ì¶œ í•¨ìˆ˜
async function callOpenAI(config: AIConfig, messages: AIMessage[]) {
  const response = await fetch(config.endpoint!, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`
    },
    body: JSON.stringify({
      model: config.model,
      messages,
      temperature: 0.7,
      max_tokens: 500
    })
  })

  const data = await response.json()
  return data.choices[0].message.content
}

async function callAnthropic(config: AIConfig, messages: AIMessage[]) {
  const response = await fetch(config.endpoint!, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': config.apiKey,
      'anthropic-version': '2023-06-01'
    },
    body: JSON.stringify({
      model: config.model,
      messages: messages.filter(m => m.role !== 'system'),
      system: messages.find(m => m.role === 'system')?.content || '',
      max_tokens: 500
    })
  })

  const data = await response.json()
  return data.content[0].text
}

async function callGoogle(config: AIConfig, messages: AIMessage[]) {
  const endpoint = `${config.endpoint}/${config.model}:generateContent?key=${config.apiKey}`
  
  const response = await fetch(endpoint, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      contents: messages.map(m => ({
        role: m.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: m.content }]
      }))
    })
  })

  const data = await response.json()
  return data.candidates[0].content.parts[0].text
}

async function callGroq(config: AIConfig, messages: AIMessage[]) {
  // GroqëŠ” OpenAI í˜¸í™˜ API
  return callOpenAI(config, messages)
}

async function callCustomAPI(config: AIConfig, messages: AIMessage[]) {
  const response = await fetch(config.endpoint!, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`
    },
    body: JSON.stringify({
      model: config.model,
      messages,
      // ì‚¬ìš©ìê°€ ì •ì˜í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
      ...JSON.parse(process.env.NEXT_PUBLIC_AI_CUSTOM_PARAMS || '{}')
    })
  })

  const data = await response.json()
  // ì‘ë‹µ êµ¬ì¡°ëŠ” ì‚¬ìš©ì ì •ì˜ APIì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
  return data.response || data.choices?.[0]?.message?.content || data.content
}

// ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
function generateRuleBasedResponse(message: string): string {
  const lowerMessage = message.toLowerCase()
  
  // ê°ì • ê´€ë ¨
  if (lowerMessage.includes('í”¼ê³¤') || lowerMessage.includes('í˜ë“¤')) {
    return 'í”¼ê³¤í•˜ì‹œêµ°ìš”. ğŸ˜” ì¶©ë¶„í•œ íœ´ì‹ë„ ì¤‘ìš”í•œ ì„±ì¥ì˜ ì¼ë¶€ì˜ˆìš”!\n\nğŸ’¡ ì¶”ì²œ í™œë™:\nâ€¢ 10ë¶„ ëª…ìƒí•˜ê¸° (ğŸ§˜ ê±´ê°• +10 EXP)\nâ€¢ ì¢‹ì•„í•˜ëŠ” ìŒì•… ë“£ê¸° (ğŸµ ê±´ê°• +5 EXP)\nâ€¢ ì¼ì° ì ë“¤ê¸° (ğŸ˜´ ê±´ê°• +15 EXP)'
  }
  
  if (lowerMessage.includes('í–‰ë³µ') || lowerMessage.includes('ê¸°ë»')) {
    return 'í–‰ë³µí•˜ì‹  ëª¨ìŠµì´ ë³´ê¸° ì¢‹ë„¤ìš”! ğŸ˜Š\n\nì´ëŸ° ê¸ì •ì ì¸ ì—ë„ˆì§€ë¥¼ í™œìš©í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?\nâ€¢ ê°ì‚¬ ì¼ê¸° ì“°ê¸° (ğŸ“ í•™ìŠµ +10 EXP)\nâ€¢ ì¹œêµ¬ì™€ ê¸°ì¨ ë‚˜ëˆ„ê¸° (ğŸ’¬ ê´€ê³„ +15 EXP)\nâ€¢ ìƒˆë¡œìš´ ë„ì „í•˜ê¸° (ğŸ¯ ì„±ì·¨ +20 EXP)'
  }

  // í™œë™ ê´€ë ¨
  if (lowerMessage.includes('ìš´ë™')) {
    return 'ìš´ë™ì„ í•˜ì…¨êµ°ìš”! ğŸ’ª í›Œë¥­í•´ìš”!\n\nğŸƒ ê±´ê°• ìŠ¤íƒ¯ì´ ì„±ì¥í–ˆì–´ìš”! (+20 EXP)\n\nì¶”ê°€ë¡œ ì´ëŸ° í™œë™ë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”:\nâ€¢ ìš´ë™ í›„ ìŠ¤íŠ¸ë ˆì¹­ (ê±´ê°• +5 EXP)\nâ€¢ ìš´ë™ ê¸°ë¡ ì‘ì„± (í•™ìŠµ +10 EXP)\nâ€¢ ìš´ë™ ì‚¬ì§„ ê³µìœ  (ê´€ê³„ +10 EXP)'
  }

  // ê¸°ë³¸ ì‘ë‹µ
  return 'ë” ìì„¸íˆ ë§ì”€í•´ì£¼ì‹œë©´ ë” ë‚˜ì€ ì¡°ì–¸ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš”! ğŸ˜Š\n\nì˜ˆë¥¼ ë“¤ì–´:\nâ€¢ "ì˜¤ëŠ˜ ìš´ë™ì„ 30ë¶„ í–ˆì–´ìš”"\nâ€¢ "ê³µë¶€ê°€ ì˜ ì•ˆë¼ìš”"\nâ€¢ "ì¹œêµ¬ì™€ ì‹œê°„ì„ ë³´ëƒˆì–´ìš”"\n\nì–´ë–¤ ì´ì•¼ê¸°ë“  í¸í•˜ê²Œ ë‚˜ëˆ ì£¼ì„¸ìš”!'
}