'use client'

import React, { useState, useEffect, useCallback, useRef } from 'react'
import { useActivitySpeechRecognition } from '@/lib/speech/use-speech-recognition'
import { cn } from '@/lib/utils'
import { VoiceInputIcon } from './VoiceInputIcon'
import { VoiceInputFallback } from './VoiceInputFallback'
import { StatSelectionModal } from './StatSelectionModal'
import { StatType } from '@/lib/types/dashboard'
import { motion, AnimatePresence } from 'framer-motion'

interface EnhancedVoiceInputProps {
  onTranscript?: (transcript: string, activityType?: string | null) => void
  onError?: (error: Error) => void
  className?: string
  position?: 'bottom-right' | 'bottom-center' | 'bottom-left'
}

const MAX_RECORDING_TIME = 60 // ìµœëŒ€ ë…¹ìŒ ì‹œê°„ (ì´ˆ)

export const EnhancedVoiceInput = React.memo(function EnhancedVoiceInput({
  onTranscript,
  onError,
  className,
  position = 'bottom-center'
}: EnhancedVoiceInputProps) {
  const [showStatSelection, setShowStatSelection] = useState(false)
  const [selectedStat, setSelectedStat] = useState<{ type: string; name: string; emoji: string } | null>(null)
  const [showFallback, setShowFallback] = useState(false)
  const [recordingTime, setRecordingTime] = useState(0)
  const [pulseAnimation, setPulseAnimation] = useState(false)
  const processingRef = useRef(false)
  const processedTranscriptsRef = useRef<Set<string>>(new Set())
  
  // âš ï¸ ìŒì„± ì…ë ¥ ì™„ë£Œ ë°ì´í„°ë¥¼ ë³„ë„ë¡œ ê´€ë¦¬ (2íšŒ ì‹œë„ ë²„ê·¸ ë°©ì§€)
  const [completedVoiceData, setCompletedVoiceData] = useState<{
    transcript: string
    statType: string
    timestamp: number
  } | null>(null)

  const {
    isSupported,
    isListening,
    status,
    transcript,
    error,
    start,
    stop,
    reset
  } = useActivitySpeechRecognition()

  // transcript ë³€í™” ì¶”ì 
  useEffect(() => {
    console.log('ğŸ“ EnhancedVoiceInput: transcript ë³€í™” ê°ì§€', {
      transcript,
      length: transcript.length,
      isListening,
      status,
      timestamp: new Date().toISOString()
    })
  }, [transcript, isListening, status])

  // ìœ„ì¹˜ ìŠ¤íƒ€ì¼
  const positionStyles = {
    'bottom-right': 'bottom-24 right-6',
    'bottom-center': 'bottom-24 left-1/2 -translate-x-1/2',
    'bottom-left': 'bottom-24 left-6'
  }

  // ë…¹ìŒ ì‹œê°„ íƒ€ì´ë¨¸
  useEffect(() => {
    let interval: NodeJS.Timeout

    if (isListening) {
      interval = setInterval(() => {
        setRecordingTime(prev => {
          if (prev >= MAX_RECORDING_TIME - 1) {
            stop()
            return 0
          }
          return prev + 1
        })
      }, 1000)
    } else {
      setRecordingTime(0)
    }

    return () => clearInterval(interval)
  }, [isListening, stop])

  // ìŒì„± ì…ë ¥ ì‹œì‘
  const handleVoiceButtonClick = useCallback(async () => {
    if (!isSupported) {
      console.log('âš ï¸ EnhancedVoiceInput: Web Speech API ë¯¸ì§€ì›')
      setShowFallback(true)
      return
    }

    // ë§ˆì´í¬ ê¶Œí•œ í™•ì¸
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      console.log('âœ… EnhancedVoiceInput: ë§ˆì´í¬ ê¶Œí•œ ìŠ¹ì¸ë¨')
      stream.getTracks().forEach(track => track.stop()) // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
    } catch (err) {
      console.error('âŒ EnhancedVoiceInput: ë§ˆì´í¬ ê¶Œí•œ ê±°ë¶€ë¨:', err)
      if (onError) {
        onError(new Error('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.'))
      }
      setShowFallback(true)
      return
    }

    if (isListening) {
      stop()
    } else {
      // í•­ìƒ ìŠ¤íƒ¯ ì„ íƒ ëª¨ë‹¬ í‘œì‹œ
      setShowStatSelection(true)
    }
  }, [isSupported, isListening, stop])

  // ìŠ¤íƒ¯ ì„ íƒ í›„ ë…¹ìŒ ì‹œì‘
  const handleStatSelect = useCallback(async(statType: { type: string; name: string; emoji: string }) => {
    console.log('ğŸ¯ EnhancedVoiceInput: ìŠ¤íƒ¯ ì„ íƒë¨', statType)
    setSelectedStat(statType)
    setShowStatSelection(false)

    try {
      console.log('ğŸ¤ EnhancedVoiceInput: ìŒì„± ì¸ì‹ ì‹œì‘ ì‹œë„...')
      await start()
      setPulseAnimation(true)
      console.log('âœ… EnhancedVoiceInput: ìŒì„± ì¸ì‹ ì‹œì‘ ì„±ê³µ')
    } catch (err) {
      console.error('âŒ EnhancedVoiceInput: ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:', err)
      if (onError) {
        onError(err instanceof Error ? err : new Error(String(err)))
      }
    }
  }, [start, onError])

  // ë…¹ìŒ ì™„ë£Œ í›„ ì²˜ë¦¬ - ì™„ë£Œ ë°ì´í„° ì €ì¥ë§Œ ìˆ˜í–‰
  useEffect(() => {
    console.log('ğŸ¤ EnhancedVoiceInput effect triggered:', {
      isListening,
      transcript,
      selectedStat,
      hasOnTranscript: !!onTranscript
    })
    
    if (!isListening && transcript && selectedStat && !processingRef.current) {
      // ì´ë¯¸ ì²˜ë¦¬ëœ transcriptì¸ì§€ í™•ì¸
      const transcriptKey = `${transcript}-${selectedStat.type}-${Date.now()}`
      if (processedTranscriptsRef.current.has(transcript)) {
        console.log('âš ï¸ EnhancedVoiceInput - ì´ë¯¸ ì²˜ë¦¬ëœ transcript, ë¬´ì‹œ:', transcript)
        return
      }
      
      console.log('ğŸ¤ EnhancedVoiceInput - Saving completed data:', {
        transcript,
        statType: selectedStat.type,
        statName: selectedStat.name,
        timestamp: new Date().toISOString()
      })
      
      // ì²˜ë¦¬ ì¤‘ í”Œë˜ê·¸ ì„¤ì •
      processingRef.current = true
      processedTranscriptsRef.current.add(transcript)
      
      // ì™„ë£Œ ë°ì´í„° ì €ì¥ (ì‹¤ì œ ì²˜ë¦¬ëŠ” ë³„ë„ effectì—ì„œ)
      setCompletedVoiceData({
        transcript: transcript,
        statType: selectedStat.type,
        timestamp: Date.now()
      })
      
      // 1ì´ˆ í›„ ë‹¤ì‹œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡
      setTimeout(() => {
        processingRef.current = false
      }, 1000)
      
      // 5ì´ˆ í›„ ì²˜ë¦¬ëœ transcript ëª©ë¡ì—ì„œ ì œê±° (ì¬ì‹œë„ ê°€ëŠ¥í•˜ë„ë¡)
      setTimeout(() => {
        processedTranscriptsRef.current.delete(transcript)
      }, 5000)
    }
  }, [isListening, transcript, selectedStat])
  
  // âš ï¸ ì™„ë£Œëœ ìŒì„± ë°ì´í„° ì²˜ë¦¬ (ë³„ë„ effectë¡œ ë¶„ë¦¬í•˜ì—¬ race condition ë°©ì§€)
  useEffect(() => {
    if (completedVoiceData && onTranscript) {
      console.log('ğŸ¤ EnhancedVoiceInput - Processing completed voice data:', {
        ...completedVoiceData,
        timestamp: new Date().toISOString()
      })
      
      // ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ async í•¨ìˆ˜
      const processVoiceData = async () => {
        try {
          // onTranscript í˜¸ì¶œ (Promiseì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ await)
          await onTranscript(completedVoiceData.transcript, completedVoiceData.statType)
          
          console.log('âœ… EnhancedVoiceInput - Voice data processed successfully')
          
          // ì²˜ë¦¬ ì™„ë£Œ í›„ì—ë§Œ ìƒíƒœ ì´ˆê¸°í™”
          reset()
          setSelectedStat(null)
          setCompletedVoiceData(null)
        } catch (error) {
          console.error('âŒ EnhancedVoiceInput - Error processing voice data:', error)
          if (onError) {
            onError(error instanceof Error ? error : new Error('ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ'))
          }
        }
      }
      
      processVoiceData()
    }
  }, [completedVoiceData]) // âš ï¸ ì˜ì¡´ì„± ë°°ì—´ ìˆ˜ì •: í•¨ìˆ˜ë“¤ì€ ì œì™¸í•˜ê³  ë°ì´í„°ë§Œ í¬í•¨

  // ì—ëŸ¬ ë°œìƒ ì‹œ í´ë°± UI í‘œì‹œ
  useEffect(() => {
    if (error) {
      console.error('ğŸš¨ EnhancedVoiceInput: ìŒì„± ì¸ì‹ ì—ëŸ¬ ë°œìƒ', error)
      setShowFallback(true)
      setSelectedStat(null)
    }
  }, [error])

  // ìŒì„± ì¸ì‹ íƒ€ì„ì•„ì›ƒ ê°ì§€ (10ì´ˆ ë™ì•ˆ transcript ì—†ìœ¼ë©´ í´ë°±)
  useEffect(() => {
    let timeoutId: NodeJS.Timeout
    
    if (isListening && selectedStat) {
      timeoutId = setTimeout(() => {
        console.log('â±ï¸ EnhancedVoiceInput: ìŒì„± ì¸ì‹ íƒ€ì„ì•„ì›ƒ - í…ìŠ¤íŠ¸ ì…ë ¥ìœ¼ë¡œ ì „í™˜')
        stop()
        setShowFallback(true)
        setSelectedStat(null)
      }, 10000) // 10ì´ˆ íƒ€ì„ì•„ì›ƒ
    }

    return () => clearTimeout(timeoutId)
  }, [isListening, selectedStat, stop])

  // í„ìŠ¤ ì• ë‹ˆë©”ì´ì…˜ íƒ€ì´ë¨¸
  useEffect(() => {
    if (pulseAnimation && !isListening) {
      const timer = setTimeout(() => {
        setPulseAnimation(false)
      }, 1000)
      return () => clearTimeout(timer)
    }
  }, [pulseAnimation, isListening])

  // ì§ì ‘ ì…ë ¥ ì²˜ë¦¬
  const handleManualSubmit = useCallback((input: string) => {
    if (onTranscript) {
      onTranscript(input, null)
    }
    setShowFallback(false)
  }, [onTranscript])

  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60)
    const secs = seconds % 60
    return `${mins}:${secs.toString().padStart(2, '0')}`
  }

  return (
    <>
      {/* ë©”ì¸ ë²„íŠ¼ */}
      <div className={cn('fixed z-40', positionStyles[position])}>
        <button
          onClick={handleVoiceButtonClick}
          disabled={false}
          className={cn(
            'p-4 bg-indigo-600 dark:bg-indigo-500',
            'text-white rounded-full shadow-lg',
            'hover:bg-indigo-700 dark:hover:bg-indigo-600',
            'disabled:bg-gray-400 dark:disabled:bg-gray-600',
            'transition-all duration-300 transform',
            'hover:scale-110 active:scale-95',
            'focus:outline-none focus:ring-4 focus:ring-indigo-300',
            'dark:focus:ring-indigo-700',
            className
          )}
          aria-label={isListening ? 'ìŒì„± ë…¹ìŒ ì¤‘ì§€' : 'ìŒì„±ìœ¼ë¡œ í™œë™ ê¸°ë¡'}
        >
          <VoiceInputIcon
            isSupported={isSupported}
            error={error}
            isListening={isListening}
            status={status}
            pulseAnimation={pulseAnimation}
          />
        </button>
        
      </div>

      {/* ë…¹ìŒ ì¤‘ ìƒíƒœ í‘œì‹œ */}
      <AnimatePresence>
        {isListening && selectedStat && (
          <motion.div
            initial={{ opacity: 0, y: 20 }}
            animate={{ opacity: 1, y: 0 }}
            exit={{ opacity: 0, y: 20 }}
            className={cn(
              'fixed z-30 bg-white dark:bg-gray-800 rounded-2xl shadow-xl p-4',
              positionStyles[position],
              'transform -translate-y-20'
            )}
          >
            <div className="flex items-center gap-3">
              <div className="text-3xl">{selectedStat.emoji}</div>
              <div className="flex-1">
                <div className="text-sm font-medium text-gray-700 dark:text-gray-300">
                  {selectedStat.name} ë…¹ìŒ ì¤‘...
                </div>
                <div className="text-lg font-bold text-gray-900 dark:text-gray-100">
                  {formatTime(recordingTime)} / {formatTime(MAX_RECORDING_TIME)}
                </div>
              </div>
              <div className="relative w-12 h-12">
                <svg className="w-12 h-12 transform -rotate-90">
                  <circle
                    cx="24"
                    cy="24"
                    r="20"
                    stroke="currentColor"
                    strokeWidth="4"
                    fill="none"
                    className="text-gray-200 dark:text-gray-700"
                  />
                  <circle
                    cx="24"
                    cy="24"
                    r="20"
                    stroke="currentColor"
                    strokeWidth="4"
                    fill="none"
                    strokeDasharray={`${(recordingTime / MAX_RECORDING_TIME) * 126} 126`}
                    className="text-indigo-600 dark:text-indigo-500 transition-all duration-1000"
                  />
                </svg>
                <div className="absolute inset-0 flex items-center justify-center">
                  <div className="w-3 h-3 bg-red-500 rounded-full animate-pulse" />
                </div>
              </div>
            </div>
          </motion.div>
        )}
      </AnimatePresence>

      {/* ìŠ¤íƒ¯ ì„ íƒ ëª¨ë‹¬ */}
      <StatSelectionModal
        isOpen={showStatSelection}
        onClose={() => setShowStatSelection(false)}
        onSelectStat={handleStatSelect}
      />

      {/* ì§ì ‘ ì…ë ¥ UI */}
      <VoiceInputFallback
        show={showFallback}
        position={position}
        onSubmit={handleManualSubmit}
        onClose={() => setShowFallback(false)}
      />
    </>
  )
})
